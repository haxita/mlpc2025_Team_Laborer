{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afde78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech: positive rate = 9.05%\n",
      "Dog Bark: positive rate = 2.04%\n",
      "Siren: positive rate = 1.66%\n",
      "Rain: positive rate = 5.56%\n",
      "Car: positive rate = 6.38%\n",
      "\n",
      "Mean positive rate: 4.94%\n",
      "Baseline accuracy (always negative) ≈ 95.06%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "split_df = pd.read_csv(\n",
    "    '/home/mtang/vslib/mlpc2025_Team_Laborer/notebooks/Task3/tang/data_split_tang.csv'\n",
    ")\n",
    "val_files = split_df[split_df['split']=='val']['filename'].tolist()\n",
    "\n",
    "\n",
    "subset = ['Speech','Dog Bark','Siren','Rain','Car']\n",
    "label_dir = '/home/mtang/vslib/mlpc2025_Team_Laborer/MLPC2025_classification/labels'\n",
    "\n",
    "pos_rates = []\n",
    "for cls in subset:\n",
    "    pos = 0\n",
    "    total = 0\n",
    "    for fn in val_files:\n",
    "        labs = np.load(os.path.join(label_dir, fn.replace('.mp3','_labels.npz')))[cls]\n",
    "        votes = labs.sum(axis=1) >= np.ceil(labs.shape[1]/2)\n",
    "        pos += votes.sum()\n",
    "        total += len(votes)\n",
    "    pos_rate = pos / total\n",
    "    pos_rates.append(pos_rate)\n",
    "    print(f\"{cls}: positive rate = {pos_rate:.2%}\")\n",
    "\n",
    "mean_pos = np.mean(pos_rates)\n",
    "print(f\"\\nMean positive rate: {mean_pos:.2%}\")\n",
    "print(f\"Baseline accuracy (always negative) ≈ {1-mean_pos:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d5ec8",
   "metadata": {},
   "source": [
    "I will use Macro-F1 as the evaluation metric. It balances precision and recall, let alone accuracy. Our sample data (5 classes) have around 95% always negative frame. That means, if use Accuracy and choose no event ever happen we can get up to 95% accuracy. Let's use this as the base line performace and see how our model performs against it.\n",
    "\n",
    "The best model in theroy can correctly align with the manual annotation so the macro-F1 will close to 1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111c0b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpc2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
